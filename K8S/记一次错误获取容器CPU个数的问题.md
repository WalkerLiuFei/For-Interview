## 问题描述

一个需要及时响应的接口，获取矿机的状态,算力情况。要求需要配在100 ms以内完成响应。数据存储在Redis 

因为追查接口毛刺比较复杂，我们的原则是不影响业务的情况下，尽量少上线的将业务问题解决。

**Redis 命令执行问题：**、首先排查是不是网络问题，查一段时间的 redis slowlog, 然后和本地抓包的数据进行对比。 这样就可以得出是网络问题还是Redis 命令执行问题。

**网络问题 ：** 在生产服务器上面 抓 tcpdump 的日志。 然后分析超时的key。

**负载分析：**  主要是看 k8s 里面 pod的监控（CPU），主要是看CPU占有率等等是否和响应时间对的上。

**Redis SDK实现问题** ： 因为Redis用了Pool，排查是否是 lock 和wait 的逻辑导致的，将执行栈（查询Goroutine的执行日志，没发现加速的逻辑 ）。查 redis sdk，这库我们维护的，看源码，看实时栈，看是否有阻塞（sdk 用了pool，pool 逻辑是否可能造成阻塞）。分析

第五、查看 runtime 监控，看是否有协程暴增，看 gc stw 时间是否影响 redis（go 版本有点低，同时内存占用大）；

第六、抓 trace ，看调度时间和调度时机是否有问题（并发协程数，GOMAXPROCS cpu负载都会影响调度）；



最终的问题的定位是Gomaxproc 设置过大，pod中的应用跑在容器上面，Go获取最大的CPU核心数是通过读 而通过看k8s，只为单一Pod分配了 2个核心数。获取到的是宿主机的CPU核心数，而不是容器限制的最大核心数！这样的结果是 增大了 runtime的调度复杂度。**为什么获取到错误的 cpu 数，会导致业务耗时增长这么严重？主要还是对延迟要求太敏感了，然后又是高频服务，runtime 的影响被放大了。**



## 如何解决？

在 编排文件中显示设置 最大CPU核心数的环境变量。



## 总结

对于K8s的资源获取时要注意， 它时调用的linux的