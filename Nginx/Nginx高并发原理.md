# Nginx高并发原理

首先要明白，Nginx 采用的是多进程（单线程） & 多路IO复用模型。使用了 I/O 多路复用技术的 Nginx，就成了”并发事件驱动“的服务器。

## 为什么采用多进程 单线程的工作模式

Nginx采用单一的 Master进程和多个Worker子进程组成,类似于Redis集群(单进程,单线程,但是集群相当于多进程).其实也很好理解,Nginx和Redis一样都是处理IO的,而多线程处理目的是最大限度的利用`CPU`资源,明显的Nginx和Redis的使用中,`CPU`资源不会称为瓶颈.

多进程单线程的模型避免加锁,因为每个进程有自己的一套内存区域.

## Woker进程数和亲缘性绑定

Nginx采用多进程Master/Worker结构，Worker进程数为CPU个数时工作效率最高，Nginx通过affinity为每个Worker进程绑定一个CPU，避免进程切换带来的消耗，同时能够保证缓存命中率(L1 级别缓存)。 

Nginx配置文件conf/nginx.conf中关于Worker进程个数，和affinity的配置命令：

```properties
worker_processes  4;
worker_cpu_affinity 1000 0100 0010 0001;
```

针对`NUMA`架构系统的特点，可以通过将进程/线程绑定指定CPU(一个或多个)的方式，提高CPU CACHE的命中率，减少进程/线程迁移CPU造成的内存访问的时间消耗，从而提高程序的运行效率。

Linux调度器同样支持自然CPU亲和性(natural CPU affinity): 调度器会试图保持进程在相同的CPU上运行, 这意味着进程通常不会在处理器之间频繁迁移,进程迁移的频率小就意味着产生的负载小。

## 异步/非阻塞/事件驱动

以一个反向代理为例子, 在一个worker进程处理一个`request`之后,交给`upstream`处理并注册一个事件 (`epoll`) 然后等待其返回,在这个等待过程中会处理另外的request.当`upstream`将请求处理完毕以后,主动通知 `worker`进程(同样 是 epoll). 

通过这样的异步,非阻塞, 事件驱动的方式 Nginx保证了并发的效率.

## 使用 epoll

## 搞清楚 TCP 连接的并发

### Client 端

client 端只能发起`65535` 个TCP连接,因为`Client`端发起的`TCP`连接都是独占的.

client每次发起tcp连接请求时，除非绑定端口，通常会让系统选取一个空闲的本地端口（local port），**该端口是独占的**，不能和其他tcp连接共享。tcp端口的数据类型是unsigned short，因此本地端口个数最大只有65536，端口0有特殊含义，不能使用，这样可用端口最多只有65535，所以在全部作为client端的情况下，最大tcp连接数为65535，这些连接可以连到不同的server ip。

### Server 端

**首先是对于 TCP 连接并发的掌握。**不少同学认为，由于端口最大范围是 65535，所以 Nginx 的最大并发连接数就是 65535。实际上，Nginx 的并发连接可以达到百万或者千万级，**原因是 TCP 连接是个四元组，它包括（源 IP、源端口、目的 IP、目的端口）**。 

我们以 32 位的 IPv4 地址为例，如果不考虑资源限制，理论上它的最大并发连接数是 2^(32+16+32+16) 个（因为 IP 为 32 位，端口号为 16 位），远远大于 65535。



